{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13532874,"sourceType":"datasetVersion","datasetId":8593186}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nimport os\nimport tensorflow as tf\n!pip install mafese mealpy plotly==6.3.0 cudf-cu11 cupy-cuda11x\nos.environ['XGB_USE_CPP_API'] = '0'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU device\nimport cudf","metadata":{"_uuid":"a0f39571-2d20-4ecb-ab7b-089410531d7f","_cell_guid":"88823f5b-20c2-4429-b30c-9289574e7a74","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rrmse(y_true, y_pred):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mean_y = np.mean(y_true)\n    return rmse / mean_y\n\ndef arrmse(y_true, y_pred):\n    n_targets = y_true.shape[1]\n    rrmse_scores = []\n    for i in range(n_targets):\n        rrmse_scores.append(rrmse(y_true[:, i], y_pred[:, i]))\n    return np.mean(rrmse_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/aq-bench/AQbench_dataset.csv')\nstr_columns = df.select_dtypes(include=['object']).columns.tolist()\nstr_columns = [col for col in str_columns if col != 'dataset']\nlabel_encoders = {}\nfor col in str_columns:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col].astype(str).fillna('NaN'))\n    label_encoders[col] = le\ndef sine_cosine_encode(values, period=None):\n    values_array = np.array(values)\n    if period is None:\n        period = values_array.max()\n    sin_values = np.sin(2 * np.pi * values_array / period)\n    cos_values = np.cos(2 * np.pi * values_array / period)\n    return sin_values, cos_values\ndf['lonx'], df['lony'] = sine_cosine_encode(df['lon'], period=360)\ndf = df.drop('lon', axis=1)\nvar_df = pd.read_csv('/kaggle/input/aq-bench/AQbench_variables.csv')\ninput_cols = var_df.loc[(var_df['input_target'] == 'input') & (var_df['column_name'] != 'lon'), 'column_name'].tolist()\nif 'lon' in input_cols:\n    input_cols.remove('lon')\ninput_cols += ['lonx', 'lony']\ntarget_cols = var_df.loc[var_df['input_target'] == 'target', 'column_name'].tolist()\nx_train = df[df['dataset'] == 'train'][input_cols]\ny_train = df[df['dataset'] == 'train'][target_cols]\nx_test = df[df['dataset'] == 'test'][input_cols]\ny_test = df[df['dataset'] == 'test'][target_cols]\nx_val = df[df['dataset'] == 'val'][input_cols]\ny_val = df[df['dataset'] == 'val'][target_cols]\ndf = df.drop('dataset', axis=1)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mafese.wrapper.mha import MultiMhaSelector\nselected_mhas = ['OriginalGA', 'OriginalPSO', 'OriginalACO', 'OriginalABC', 'OriginalCS', 'OriginalWOA', 'OriginalGWO']\nmha_scores = {}\nfor mha in selected_mhas:\n    print(f'Evaluating MHA: {mha}')\n    scores = []\n    for target in y_train.columns:\n        selector = MultiMhaSelector(problem='regression', obj_name='RMSE', estimator='xgb', list_optimizers=[mha], verbose=True)\n        selector.fit(x_train.values, y_train[target].values)\n        selected_indices = selector.selected_feature_indexes\n        # Fallback if no features selected by mafese\n        if len(selected_indices) == 0:\n            print(f\"[Warning] MAFESE selected zero features for target '{target}'. Using all input features as fallback.\")\n            feature_subset = input_cols\n        else:\n            feature_subset = [input_cols[i] for i in selected_indices]\n        \n        xgb_x_train = x_train[feature_subset]\n        xgb_x_val = x_val[feature_subset]\n\n        # Move data to GPU using cuDF\n        xgb_x_train_gpu = cudf.from_pandas(xgb_x_train)\n        y_train_gpu = cudf.from_pandas(y_train[target])\n        xgb_x_val_gpu = cudf.from_pandas(xgb_x_val)\n        y_val_gpu = cudf.from_pandas(y_val[target])\n\n        model = XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor', n_estimators=100)\n        model.fit(xgb_x_train_gpu, y_train_gpu)\n        preds = model.predict(xgb_x_val_gpu)\n        score = rrmse(y_val_gpu, preds)\n        scores.append(score)\n        print(f'    {target} Accuracy: {score}')\n    avg_score = np.mean(scores)\n    mha_scores[mha] = avg_score\n    print(f'{mha} average validation RRMSE: {avg_score}')\nprint('MHA evaluation results:', mha_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}